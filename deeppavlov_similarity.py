# -*- coding: utf-8 -*-
"""deeppavlov_similarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TYG20DNO8aZmy6b7rS3D97EPB2S29kTX
"""

!pip install transformers

from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("DeepPavlov/rubert-base-cased-sentence")

model = AutoModel.from_pretrained("DeepPavlov/rubert-base-cased-sentence")

import time
import torch
from sklearn.metrics.pairwise import cosine_similarity

def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0] #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


pairwise_sentences_eliza = [
    "Я пошел гулять",
    "вы пошел гулять"
]
pairwise_sentences_seq2seq = [
    "Я пошел гулять",
    "ладно пойду спокойной"
]
pairwise_sentences_rut5 = [
    "Я пошел гулять",
    "Я знаю, что ты не можешь пойти вместе?"
]
pairwise_sentences_rugpt = [
    "Я пошел гулять",
    "Вы не можете пойти погулять, потому что вы в данный момент находитесь на работе и"
]

list_paiwise_sents = [pairwise_sentences_eliza,pairwise_sentences_seq2seq,pairwise_sentences_rut5,pairwise_sentences_rugpt]
for item in list_paiwise_sents:
  print(item)
  time_start_similarity = time.time()
  encoded_input = tokenizer(item, padding=True, truncation=True, return_tensors='pt')
  with torch.no_grad():
      model_output = model(**encoded_input)
  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
  sentence_embeddings = sentence_embeddings.detach().numpy()
  cosine_array = cosine_similarity(
        [sentence_embeddings[0]],
        sentence_embeddings[1:]
    )

  print(cosine_array.sum()/(2-1))
  print("Similarity Calculation time: ", time.time() - time_start_similarity)

pairwise_sentences_eliza = [
    "Я чувствую обиду и злость за свой поступок",
    "Когда вы обычно чувствуете обиду и злость за свой?"
]
pairwise_sentences_seq2seq = [
    "Я чувствую обиду и злость за свой поступок",
    "я не люблю покушать а вообще люблю танцевать дома на работе"
]
pairwise_sentences_rut5 = [
    "Я чувствую обиду и злость за свой поступок",
    "Что это за поступок?" 
]
pairwise_sentences_rugpt = [
    "Я чувствую обиду и злость за свой поступок",
    "Здравствуйте, уважаемый Алексей! Спасибо Вам большое от всей души!!! Мы обязательно исправим Вашу ошибку"
]

list_paiwise_sents = [pairwise_sentences_eliza,pairwise_sentences_seq2seq,pairwise_sentences_rut5,pairwise_sentences_rugpt]
for item in list_paiwise_sents:
  print(item)
  time_start_similarity = time.time()
  encoded_input = tokenizer(item, padding=True, truncation=True, return_tensors='pt')
  with torch.no_grad():
      model_output = model(**encoded_input)
  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
  sentence_embeddings = sentence_embeddings.detach().numpy()
  cosine_array = cosine_similarity(
        [sentence_embeddings[0]],
        sentence_embeddings[1:]
    )

  print(cosine_array.sum()/(2-1))
  print("Similarity Calculation time: ", time.time() - time_start_similarity)

pairwise_sentences_eliza = [
    "Снег автомату рознь, а собака скользкая",
    "Как ты себя чувствуешь?"
]
pairwise_sentences_seq2seq = [
    "Снег автомату рознь, а собака скользкая",
    "у меня две собаки и трое детей а там уже кот"
]
pairwise_sentences_rut5 = [
    "Снег автомату рознь, а собака скользкая",
    "О, ну что ж. Какая собака?"
]
pairwise_sentences_rugpt = [
    "Снег автомату рознь, а собака скользкая",
    "Да нет же! У меня была такая проблема с собакой - она не могла пройти"
]

list_paiwise_sents = [pairwise_sentences_eliza,pairwise_sentences_seq2seq,pairwise_sentences_rut5,pairwise_sentences_rugpt]
for item in list_paiwise_sents:
  print(item)
  time_start_similarity = time.time()
  encoded_input = tokenizer(item, padding=True, truncation=True, return_tensors='pt')
  with torch.no_grad():
      model_output = model(**encoded_input)
  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
  sentence_embeddings = sentence_embeddings.detach().numpy()
  cosine_array = cosine_similarity(
        [sentence_embeddings[0]],
        sentence_embeddings[1:]
    )

  print(cosine_array.sum()/(2-1))
  print("Similarity Calculation time: ", time.time() - time_start_similarity)

pairwise_sentences_eliza = [
    "Скажи мне что-либо приятное",
    "Как ты себя чувствуешь?"
]
pairwise_sentences_seq2seq = [
    "Скажи мне что-либо приятное",
    "оу интересно конечно но мне очень нравиться"
]
pairwise_sentences_rut5 = [
    "Скажи мне что-либо приятное",
    "Что это?"
]
pairwise_sentences_rugpt = [
    "Скажи мне что-либо приятное",
    "Я хочу, чтобы ты была счастлива и довольна своей жизнью! Ты должна быть счастливой в настоящем моменте"
]

list_paiwise_sents = [pairwise_sentences_eliza,pairwise_sentences_seq2seq,pairwise_sentences_rut5,pairwise_sentences_rugpt]
for item in list_paiwise_sents:
  print(item)
  time_start_similarity = time.time()
  encoded_input = tokenizer(item, padding=True, truncation=True, return_tensors='pt')
  with torch.no_grad():
      model_output = model(**encoded_input)
  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
  sentence_embeddings = sentence_embeddings.detach().numpy()
  cosine_array = cosine_similarity(
        [sentence_embeddings[0]],
        sentence_embeddings[1:]
    )

  print(cosine_array.sum()/(2-1))
  print("Similarity Calculation time: ", time.time() - time_start_similarity)